[
  {
    "objectID": "tutorials/tutorial_04.html",
    "href": "tutorials/tutorial_04.html",
    "title": "Tutorial 04: Diagnostics",
    "section": "",
    "text": "There are many ways to check for model robustness (the concept of “robustness” is not a mathematical one, so you can use a bunch of heuristics to assess robustness).\nGo back to the models you fitted in the tutorials 1-3 and check them with the following methods:\n\nUse plot() to have a look at the chain plots. Do they look like “hairy caterpillars”?\nPlot the cumulative posterior distribution with pp_check(). Do the simulated posteriors (in light blue) look like the empirical distribution (in dark blue)?\nCheck the \\(\\hat{R}\\) values using summary(). Are they close to one? Did you get any warnings regarding \\(\\hat{R}\\) or ESS?"
  },
  {
    "objectID": "tutorials/tutorial_04.html#diagnostics",
    "href": "tutorials/tutorial_04.html#diagnostics",
    "title": "Tutorial 04: Diagnostics",
    "section": "",
    "text": "There are many ways to check for model robustness (the concept of “robustness” is not a mathematical one, so you can use a bunch of heuristics to assess robustness).\nGo back to the models you fitted in the tutorials 1-3 and check them with the following methods:\n\nUse plot() to have a look at the chain plots. Do they look like “hairy caterpillars”?\nPlot the cumulative posterior distribution with pp_check(). Do the simulated posteriors (in light blue) look like the empirical distribution (in dark blue)?\nCheck the \\(\\hat{R}\\) values using summary(). Are they close to one? Did you get any warnings regarding \\(\\hat{R}\\) or ESS?"
  },
  {
    "objectID": "tutorials/tutorial_04.html#challenge",
    "href": "tutorials/tutorial_04.html#challenge",
    "title": "Tutorial 04: Diagnostics",
    "section": "Challenge",
    "text": "Challenge\nIf you feel like it and have time, try and run other models either using the data provided or with your own data if you have some.\nI’ll be happy to answer your questions!"
  },
  {
    "objectID": "tutorials/tutorial_02.html",
    "href": "tutorials/tutorial_02.html",
    "title": "Tutorial 02: Interactions",
    "section": "",
    "text": "The polite data also has information on the gender of the participant (F for female and M for male).\nLet’s plot again f0mn and attitude (register), this time including gender.\n\npolite %&gt;%\n  ggplot(aes(gender, f0mn, colour = attitude)) +\n  geom_jitter(position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.7))\n\n\n\n\nThe plot suggests that f0mn might be somewhat lower in the polite register in female participants, but not really in male participants.\nHow do we account for this potential interaction between register and gender?"
  },
  {
    "objectID": "tutorials/tutorial_02.html#gender-and-register",
    "href": "tutorials/tutorial_02.html#gender-and-register",
    "title": "Tutorial 02: Interactions",
    "section": "",
    "text": "The polite data also has information on the gender of the participant (F for female and M for male).\nLet’s plot again f0mn and attitude (register), this time including gender.\n\npolite %&gt;%\n  ggplot(aes(gender, f0mn, colour = attitude)) +\n  geom_jitter(position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.7))\n\n\n\n\nThe plot suggests that f0mn might be somewhat lower in the polite register in female participants, but not really in male participants.\nHow do we account for this potential interaction between register and gender?"
  },
  {
    "objectID": "tutorials/tutorial_02.html#interactions",
    "href": "tutorials/tutorial_02.html#interactions",
    "title": "Tutorial 02: Interactions",
    "section": "Interactions",
    "text": "Interactions\nWhen there is the potential for two or more predictors in a model to “interact” with each other, you should include a so-called interaction.\nHere’s how:\n\nf0mn_bm_2 &lt;- brm(\n  f0mn ~ attitude + gender + attitude:gender,\n  family = gaussian(),\n  data = polite,\n  backend = \"cmdstanr\"\n)\n\nIn the model’s formula we have attitude, gender and something new: attitude:gender.\nThe term attitude:gender is an interaction: it tells the model to allow attitude and gender to interact, so that if the effect of one predictor differs depending on the effect of the other, the model will be able to capture this.\nIn other words, if the effect of attitude differs by gender, we need to include an interaction in the model to capture this. You normally always include interactions between predictors and the results of the interaction term will help you understand if there is an interaction or not.\nNow, this is the model summary.\n\nsummary(f0mn_bm_2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: f0mn ~ attitude + gender + attitude:gender \n   Data: polite (Number of observations: 212) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept             256.72      5.05   246.93   266.64 1.00     2684     2783\nattitudepol           -17.56      7.08   -31.21    -3.72 1.00     2482     2834\ngenderM              -119.61      7.64  -134.92  -104.68 1.00     2170     2544\nattitudepol:genderM     6.70     10.66   -13.83    26.99 1.00     1989     2731\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    39.11      1.97    35.49    43.19 1.00     3837     2599\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s go through the population-level effects:\n\nIntercept: this is the mean f0mn when [attitude=informal] and [gender=F]. There is a 95% probability that this is between 247 and 266 hz.\nattitudepol: this is the difference in mean f0mn when you go from the Intercept to [attitude=polite] (so [gender=F]). We can be 95% confident that this difference is between -31 and -4 hz. In other words, when the register is polite, f0mean lowers by 4-31 hz in female participants at 95% probability.\ngenderM: this is the difference in mean f0mn when you go from the Intercept to [gender=M] (so [attitude=inf]). There is a 95% probability that this difference is between -135 and -105 hz.\nNow, the interaction term attitudepol:genderM: this tells you the adjustment you need to make to the effect of attitudepol to get the effect when [gender=M].\n\nWhat does it mean? It means that, on average, the effect of attitude is 6.7 hz smaller in males than in females: -17.56 + 6.7 = -10.86. However, there is also the possibility that the effect is 0 or that it is positive rather than negative, because the 95% CrI go from -14 to +27 hz."
  },
  {
    "objectID": "tutorials/tutorial_02.html#plotting-the-model-results",
    "href": "tutorials/tutorial_02.html#plotting-the-model-results",
    "title": "Tutorial 02: Interactions",
    "section": "Plotting the model results",
    "text": "Plotting the model results\nWe can visualise the results using the conditional_effects() function.\n\nconditional_effects(f0mn_bm_2, \"gender:attitude\")\n\n\n\n\nThe effect of attitude is clearer in female participants than in males, although it is still possible that male participants lower their mean f0 when speaking politely."
  },
  {
    "objectID": "tutorials/tutorial_02.html#fomula-of-an-interaction",
    "href": "tutorials/tutorial_02.html#fomula-of-an-interaction",
    "title": "Tutorial 02: Interactions",
    "section": "Fomula of an interaction",
    "text": "Fomula of an interaction\nWe can understand what the interaction term is doing through the formula of the model we fitted above. Feel free to skip this part if you find that formulae don’t help you.\n\\[\n\\begin{aligned}\nf0mn & \\sim Gaussian(\\mu, \\sigma) \\\\\n\\\\\n\\mu & \\sim \\beta_0 + \\beta_1 \\times attitude_{pol} + \\beta_2 \\times gender_{M} + \\beta_3 \\times attitude_{pol} \\times gender_{M} \\\\\n\\sigma & \\sim Gaussian_+(39, 1.97)\n\\end{aligned}\n\\]\nWhen the [attitude=polite], \\(attitude_{pol}\\) is 1 and when [gender=M], \\(gender_M\\) is 1, otherwise they are 0.\nNow, if you want to get \\(\\mu\\) (i.e. the mean f0mn) for a particular register/gender, you just fill in the estimates from the model summary:\n\n\\(\\beta_0\\) is the Intercept.\n\\(\\beta_2\\) is attitudepol.\n\\(\\beta_3\\) is genderM.\n\\(\\beta_4\\) is attitudepol:genderM\n\nLet me show you the process for the mean f0mn in the polite register and female participant.\n\\[\n\\begin{aligned}\n\\mu_{[polite, F]} = & \\beta_0 + \\beta_1 \\times attitude_{pol} + \\beta_2 \\times gender_{M} + \\beta_3 \\times attitude_{pol} \\times gender_{M} \\\\\n= & 256.72 + (-17.56) \\times 1 + (-119.61) \\times 0 + 6.7 \\times 1 \\times 0 \\\\\n= & 256.72 + (-17.56) \\times 1 \\\\\n= & 239.16\n\\end{aligned}\n\\] So, the mean f0mn when the register is polite and the gender is female is about 239 hz.\nTry and figure out the mean f0mn for the other combinations of register and gender."
  },
  {
    "objectID": "tutorials/tutorial_02.html#challenge",
    "href": "tutorials/tutorial_02.html#challenge",
    "title": "Tutorial 02: Interactions",
    "section": "Challenge",
    "text": "Challenge\nIf you feel like and you have the time, try running the model again but this time use musicstudent (whether the participant studied music or not) and months_ger (how many months the participant have spent in Germany).\nRemember to include an interaction between the two predictors. A short cut is to write musicstudent * months_ger instead of the longer musicstudent + months_ger + musicstudent:months_ger."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Pre-workshop set-up",
    "section": "",
    "text": "Please, follow these instructions to get ready before the workshop."
  },
  {
    "objectID": "setup.html#pre-requisites",
    "href": "setup.html#pre-requisites",
    "title": "Pre-workshop set-up",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nBefore installing the necessary software, make sure you have installed or updated the following software.\n\nThe latest version of R (https://cloud.r-project.org).\nThe latest version of RStudio (https://www.rstudio.com/products/rstudio/download/#download).\nYour operating system is up-to-date."
  },
  {
    "objectID": "setup.html#starter-kit",
    "href": "setup.html#starter-kit",
    "title": "Pre-workshop set-up",
    "section": "Starter Kit",
    "text": "Starter Kit\nPlease, download the Starter Kit.\nYou can get the Starter Kit from here. It is just an RStudio project with some useful files."
  },
  {
    "objectID": "setup.html#installation",
    "href": "setup.html#installation",
    "title": "Pre-workshop set-up",
    "section": "Installation",
    "text": "Installation\n\n\n\n\n\n\nImportant\n\n\n\nIf you have previously installed the C++ toolkit, CmdStan and/or cmdstanr, or if you have recently updated your OS, please follow these instructions to reinstall them.\n\n\nNow you will need to install a few packages and extra software.\nHere is an overview of what you will install:\n\nC++ toolchain.\nR packages: tidyverse, remotes, cmdstanr, brms.\nCmdStan (including Stan).\n\n\nInstall the C++ toolchain\nSome of the software (CmdStan) used in the workshop requires a working C++ toolchain for compilation.\nYou can find information on how to set up the C++ toolchain in Section 1.2.1 of the CmdStan User’s Guide.\nMake sure to follow the instructions for your operating system. If you have Windows, installing RTools 42 seems to work better than installing RTools 40\n\n\nInstall the R packages\nYou need to install the following packages:\ninstall.packages(c(\"tidyverse\", \"remotes\"))\nremotes::install_github(\"stan-dev/cmdstanr\")\ninstall.packages(c(\"brms\"))\ninstall.packages(c(\"tidybayes\", \"extraDistr\"))\nThe cmdstanr package is an interface between R and CmdStan (see below), while brms is the package you will use to run Bayesian linear models (think of it as the Bayesian equivalent of lme4).\nIt will take several minutes to install the packages, depending on your system and configuration.\nIf after opening the Starter Kit in RStudio you get asked to install extra packages or software, please do so.\n\n\nInstall CmdStan\nNow that you have installed the R packages, you need to install CmdStan.\nCmdStan is a shell interface to the programming language Stan.\nStan is what runs the Bayesian models, but you don’t have to know Stan, because you will use the R packages to run the models (those packages will communicate with Stan in your stead).\nTo install CmdStan, run the following command in the R console:\ncmdstanr::install_cmdstan(cores = parallel::detectCores(), overwrite = TRUE)\n\n\nCheck your installation work\nRun the following in the RStudio Console. You will see text flashing.\nlibrary(cmdstanr)\nfile &lt;- file.path(cmdstan_path(), \"examples\", \"bernoulli\", \"bernoulli.stan\")\nmod &lt;- cmdstan_model(file)\nIf a mod object is created, then installation went well."
  },
  {
    "objectID": "setup.html#troubleshoot",
    "href": "setup.html#troubleshoot",
    "title": "Pre-workshop set-up",
    "section": "Troubleshoot",
    "text": "Troubleshoot\nIf you have issues with any of these steps, please get in touch with Stefano."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn Bayesian Linear Models for Beginners",
    "section": "",
    "text": "This is the website of the workshop basicBayes.\nPlease, check the Set-up page for important set-up instructions, before joining the workshop.\nThis workshop assumes you have a good command of R and tidyverse packages, but no prior experience with linear modelling is required.\nIf you wish to revise your basic R skills, we recommend the following resource: R for Data Science (online book)."
  },
  {
    "objectID": "tutorials/tutorial_01.html",
    "href": "tutorials/tutorial_01.html",
    "title": "Tutorial 01: BRM basics",
    "section": "",
    "text": "The tutorials are designed to be self-paced hands-ons to give you the chance to practice running Bayesian linear models in a semi-guided way.\nTo start, double click on the basicBayesKit.Rproj file to open the RStudio project with all the necessary files.\nIn the project folder, you will find the data in the data/ folder and a script called code.R where you will be writing your R code.\nI have pre-filled the script with some necessary code to attach a few packages and read the data."
  },
  {
    "objectID": "tutorials/tutorial_01.html#get-started",
    "href": "tutorials/tutorial_01.html#get-started",
    "title": "Tutorial 01: BRM basics",
    "section": "",
    "text": "The tutorials are designed to be self-paced hands-ons to give you the chance to practice running Bayesian linear models in a semi-guided way.\nTo start, double click on the basicBayesKit.Rproj file to open the RStudio project with all the necessary files.\nIn the project folder, you will find the data in the data/ folder and a script called code.R where you will be writing your R code.\nI have pre-filled the script with some necessary code to attach a few packages and read the data."
  },
  {
    "objectID": "tutorials/tutorial_01.html#the-polite-data",
    "href": "tutorials/tutorial_01.html#the-polite-data",
    "title": "Tutorial 01: BRM basics",
    "section": "The polite data",
    "text": "The polite data\nThe polite data contains several acoustic measures of the speech of Korean students living in Germany who where asked to speak using a formal (pol) or informal (inf) register (see column attitude).\n\n\n# A tibble: 212 × 27\n   subject gender birthplace musicstudent months_ger scenario task  attitude\n   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   \n 1 F1      F      seoul_area yes                  18        6 not   inf     \n 2 F1      F      seoul_area yes                  18        6 not   pol     \n 3 F1      F      seoul_area yes                  18        7 not   inf     \n 4 F1      F      seoul_area yes                  18        7 not   pol     \n 5 F1      F      seoul_area yes                  18        1 dct   pol     \n 6 F1      F      seoul_area yes                  18        1 dct   inf     \n 7 F1      F      seoul_area yes                  18        2 dct   pol     \n 8 F1      F      seoul_area yes                  18        2 dct   inf     \n 9 F1      F      seoul_area yes                  18        3 dct   pol     \n10 F1      F      seoul_area yes                  18        3 dct   inf     \n# ℹ 202 more rows\n# ℹ 19 more variables: total_duration &lt;dbl&gt;, articulation_rate &lt;dbl&gt;,\n#   f0mn &lt;dbl&gt;, f0sd &lt;dbl&gt;, f0range &lt;dbl&gt;, inmn &lt;dbl&gt;, insd &lt;dbl&gt;,\n#   inrange &lt;dbl&gt;, shimmer &lt;dbl&gt;, jitter &lt;dbl&gt;, HNRmn &lt;dbl&gt;, H1H2 &lt;dbl&gt;,\n#   breath_count &lt;dbl&gt;, filler_count &lt;dbl&gt;, hiss_count &lt;dbl&gt;,\n#   nasal_count &lt;dbl&gt;, sil_count &lt;dbl&gt;, ya_count &lt;dbl&gt;, yey_count &lt;dbl&gt;\n\n\nIn this tutorial we will focus on three variables:\n\nf0mn: the mean f0 across the entire utterance.\nattitude: informal vs polite condition.\narticulation_rate: number of syllables per second.\n\nHere’s what the data looks like.\n\n\n\n\n\nWe want now to estimate the effect of attitude and articulation_rate on f0mn.\nWe can do that using the function brm() from the brms package and fit a Bayesian linear model."
  },
  {
    "objectID": "tutorials/tutorial_01.html#brm-basics",
    "href": "tutorials/tutorial_01.html#brm-basics",
    "title": "Tutorial 01: BRM basics",
    "section": "brm() basics",
    "text": "brm() basics\nLet’s assume mean f0 values are distributed according to a Gaussian distribution (you may also have encountered Gaussian distributions under the name “normal distribution”).\n\\[f0mn \\sim Gaussian(\\mu, \\sigma)\\]\nWe want to estimate the following probability distributions:\n\\[\n\\begin{aligned}\n\\mu & \\sim Gaussian(\\mu_1, \\sigma_1) \\\\\n\\sigma & \\sim Gaussian_+(\\mu_2, \\sigma_2)\n\\end{aligned}\n\\]\nWe can achieve this by modelling the data using the brm() function from the brms package (BRM stands for Bayesian Regression Model). Note that regression is a synonym of linear model.\n\nf0mn_bm_1 &lt;- brm(\n  f0mn ~ 1,\n  family = gaussian(),\n  data = polite,\n  backend = \"cmdstanr\"\n)\n\nYou will see the message Compiling Stan program... followed by Start sampling and Running MCMC with 4 sequential chains.... Something like in the figure below.\n\nIt is OK if you don’t understand this. Stan is a programming language that can run Bayesian models. brm() is an R interface to Stan, so that you can use R and R syntax to run Bayesian models without having to learn Stan! (If you feel adventurous, nobody stops you from learning Stan too, although it is not required and you can use R to run even very complex models).\nAfter those initial messages, you will see a list of messages about chains and interactions, as in the figure below.\n\nThese are about the MCMC stuff mentioned in one of the messages above: MCMC stands for Markov-Chain Monte Carlo. Keep reading for a quick intro.\n\nMarkov-Chain Monte Carlo (MCMC)\n\n\n\n\n\n\nMCMC\n\n\n\nMarkov-Chain Monte Carlo refers to a set of algorithms used to estimate parameters of unknown distributions, based on repeated draws from parameter values and observed data.\n\n\nYou don’t need to understand the details of this. Just remember that several draws are made (i.e. the algorithm is run for several iterations), and that the model runs 4 chains of these iterations.\nYou will learn how to use information about the MCMC chains to diagnose the robustness of the estimated parameters later in the workshop.\n\n\nModel summary\nNow we can inspect the model output using the summary() function.\n\nsummary(f0mn_bm_1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: f0mn ~ 1 \n   Data: polite (Number of observations: 212) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   198.04      4.71   188.92   207.30 1.00     3270     2674\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    70.04      3.50    63.72    77.38 1.00     3016     2530\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe have seen a summary of a model where we were estimating RT values.\nThe results are in the Population-Level Effects and Family Specific Parameters parts.\nFor (Intercept) (\\(\\mu\\)):\n\nEstimate is \\(\\mu_1\\): 198 hz.\nEst.Error is \\(\\sigma_1\\): 4.71 hz.\n\nFor sd__Observation (\\(\\sigma\\)):\n\nEstimate is \\(\\mu_2\\): 70 hz.\nEst.Error is \\(\\sigma_2\\): 3.05 hz.\n\nIn other words:\n\\[\n\\begin{aligned}\nf0mn & \\sim Gaussian(\\mu, \\sigma) \\\\\n\\\\\n\\mu & \\sim Gaussian(198, 4.71) \\\\\n\\sigma & \\sim Gaussian_+(70, 3.05)\n\\end{aligned}\n\\]\nBased on the model summary, we also know the 95% Credible Intervals (or CrIs) of the probability distributions of \\(\\mu\\) and \\(sigma\\). They are under the labels l-95% CI and u-95% CI.\n\n\n\n\n\n\nCredible Interval\n\n\n\nA 95% (Bayesian) Credible Interval indicates the range of values we can be 95% confident contains the true parameter value.\n\n\n\nAt 95% confidence, the mean (\\(\\mu\\)) f0mn value is between 189 and 207 hz.\nAs for the standard deviation (\\(\\sigma\\)), we can be 95% confident that it is between 64 and 77 hz."
  },
  {
    "objectID": "tutorials/tutorial_03.html",
    "href": "tutorials/tutorial_03.html",
    "title": "Tutorial 03: Binary outcomes",
    "section": "",
    "text": "In this tutorial, you will analyse data from Song et al. 2020. Second language users exhibit shallow morphological processing. DOI: 10.1017/S0272263120000170.\n\nshallow &lt;- read_csv(\"data/shallow.csv\")\n\nRows: 6500 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Group, ID, List, Target, Critical_Filler, Word_Nonword, Relation_ty...\ndbl (3): ACC, RT, logRT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nshallow\n\n\n\n  \n\n\n\nThe study consisted of a lexical decision task in which participants where first shown a prime, followed by a target word for which they had to indicate whether it was a real word or a nonce word.\nThe prime word belonged to one of three possible groups (Relation_type in the data) each of which refers to the morphological relation of the prime and the target word:\n\nUnrelated: for example, prolong (assuming unkindness as target, [[un-kind]-ness]).\nConstituent: unkind.\nNonConstituent: kindness.\n\nThe expectation is that lexical decision for native English participants should be facilitated in the Constituent condition, but not in the Unrelated and NonConstituent conditions (if you are curious as to why that is the expectation, I refer you to the paper).\nLet’s interpret that as:\n\nThe Constituent condition should elicit better accuracy than the other two conditions.\n\nThe study compared results of English L1 vs L2 participants and of left- vs right-branching words, but for this tutorial we will only be looking at the L1 and left-branching data. The data file also contains data from the filler items, which we filter out.\nWe also mutate the ACC column.\n\nshallow_filt &lt;- shallow %&gt;%\n  filter(\n    Group == \"L1\",\n    Branching == \"Left\",\n    Critical_Filler == \"Critical\",\n  ) %&gt;%\n  mutate(\n    ACC = ifelse(ACC == 0, \"incorrect\", \"correct\")\n  )\nshallow_filt\n\n\n\n  \n\n\n\nLet’s have a look at a plot that shows accuracy based on relation type.\n\nshallow_filt %&gt;%\n  ggplot(aes(Relation_type, fill = ACC)) +\n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "tutorials/tutorial_03.html#the-shallow-data",
    "href": "tutorials/tutorial_03.html#the-shallow-data",
    "title": "Tutorial 03: Binary outcomes",
    "section": "",
    "text": "In this tutorial, you will analyse data from Song et al. 2020. Second language users exhibit shallow morphological processing. DOI: 10.1017/S0272263120000170.\n\nshallow &lt;- read_csv(\"data/shallow.csv\")\n\nRows: 6500 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Group, ID, List, Target, Critical_Filler, Word_Nonword, Relation_ty...\ndbl (3): ACC, RT, logRT\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nshallow\n\n\n\n  \n\n\n\nThe study consisted of a lexical decision task in which participants where first shown a prime, followed by a target word for which they had to indicate whether it was a real word or a nonce word.\nThe prime word belonged to one of three possible groups (Relation_type in the data) each of which refers to the morphological relation of the prime and the target word:\n\nUnrelated: for example, prolong (assuming unkindness as target, [[un-kind]-ness]).\nConstituent: unkind.\nNonConstituent: kindness.\n\nThe expectation is that lexical decision for native English participants should be facilitated in the Constituent condition, but not in the Unrelated and NonConstituent conditions (if you are curious as to why that is the expectation, I refer you to the paper).\nLet’s interpret that as:\n\nThe Constituent condition should elicit better accuracy than the other two conditions.\n\nThe study compared results of English L1 vs L2 participants and of left- vs right-branching words, but for this tutorial we will only be looking at the L1 and left-branching data. The data file also contains data from the filler items, which we filter out.\nWe also mutate the ACC column.\n\nshallow_filt &lt;- shallow %&gt;%\n  filter(\n    Group == \"L1\",\n    Branching == \"Left\",\n    Critical_Filler == \"Critical\",\n  ) %&gt;%\n  mutate(\n    ACC = ifelse(ACC == 0, \"incorrect\", \"correct\")\n  )\nshallow_filt\n\n\n\n  \n\n\n\nLet’s have a look at a plot that shows accuracy based on relation type.\n\nshallow_filt %&gt;%\n  ggplot(aes(Relation_type, fill = ACC)) +\n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "tutorials/tutorial_03.html#modeling-binary-variables",
    "href": "tutorials/tutorial_03.html#modeling-binary-variables",
    "title": "Tutorial 03: Binary outcomes",
    "section": "Modeling binary variables",
    "text": "Modeling binary variables\nAccuracy (ACC) is a binary variable and to model the probability of a binary variable we need to use the Bernoulli family. Moreover, since probabilities are bounded between 0 and 1 and linear models expect unbounded variables, the estimates of the model will be in log-odds.\nThis is how log-odds are related to probabilities.\n\n\n\n\n\nNow, fit a model with brm(). Here’s some tips:\n\nIn the formula, you want to include ACC as the outcome and Relation_type as the predictor.\nThis time, you have to specify family = bernoulli, to use the Bernoulli family (Gaussian is the default).\nRemember to use shallow_filt as the data.\n\nThen check the model summary with summary(). What does it tell you?\nCompare what you understand from the model summary with the model of the plots (use conditional_effects() to get the plot)."
  },
  {
    "objectID": "tutorials/tutorial_03.html#challenge",
    "href": "tutorials/tutorial_03.html#challenge",
    "title": "Tutorial 03: Binary outcomes",
    "section": "Challenge",
    "text": "Challenge\nIf you feel like and you have the time, try to run the model using both L1 and L2 data. You will have to include Group as a predictor and make sure you also include an interaction Group:Relation_type."
  }
]