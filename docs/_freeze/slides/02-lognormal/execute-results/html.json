{
  "hash": "cc9e023f06bd99e1b73e53e521acb7dc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"02 - Lognormal models\"\nauthor: \"Stefano Coretta\"\n---\n\n\n\n\n::: {.cell}\n\n:::\n\n\n## Is a Gaussian model a good choice?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Posterior predictive check plot of `rt_gauss`.](02-lognormal_files/figure-revealjs/fig-rt-gauss-pp-1.png){#fig-rt-gauss-pp width=960}\n:::\n:::\n\n\n## Gaussian variables are rare...\n\n::: box-note\n-   Reaction times can only be positive (negative values are excluded).\n\n-   Variables that can take only positive numbers (and are continuous) usually follow a **log-normal** distribution.\n:::\n\n## The log-normal distribution\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Log-normal distributions with varying mean and SD.](02-lognormal_files/figure-revealjs/fig-lognormal-1.png){#fig-lognormal width=960}\n:::\n:::\n\n\n## A log-normal model of RTs\n\n$$\n\\begin{align}\nRT & \\sim LogNormal(\\mu, \\sigma)\\\\\n\\end{align}\n$$\n\n::: box-note\n-   RT follow a log-normal distribution with mean $\\mu$ and standard deviation $\\sigma$.\n\n-   $\\mu$ and $sigma$ are the mean and SD of *logged* RTs.\n:::\n\n. . .\n\n::: box-tip\nequivalent to:\n\n$$\n\\begin{align}\nlog(RT) & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\end{align}\n$$\n:::\n\n## A log-normal model of RTs: code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt_logn <- brm(\n  RT ~ 1,\n  family = lognormal,\n  data = mald,\n  \n  ## Technical stuff\n  cores = 4,\n  seed = 1032,\n  file = \"data/cache/rt_logn\"\n)\n```\n:::\n\n\n## A log-normal model of RTs: posterior predictive checks\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(rt_logn, ndraws = 20)\n```\n\n::: {.cell-output-display}\n![](02-lognormal_files/figure-revealjs/fig-rt-logn-pp-1.png){#fig-rt-logn-pp width=960}\n:::\n:::\n\n\n## A log-normal model of RTs: summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(rt_logn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: lognormal \n  Links: mu = identity; sigma = identity \nFormula: RT ~ 1 \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     6.88      0.00     6.87     6.88 1.00     3517     2562\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.28      0.00     0.27     0.28 1.00     2245     2019\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## A log-normal model of RTs: MCMC draws\n\n::: box-note\n-   The MCMC draws are also called the **posterior draws**.\n\n-   They allow us to construct the **posterior probability distribution** of each model parameter.\n:::\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt_logn_draws <- as_draws_df(rt_logn)\n\nrt_logn_draws\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A draws_df: 1000 iterations, 4 chains, and 5 variables\n   b_Intercept sigma Intercept lprior   lp__\n1          6.9  0.28       6.9   -3.2 -35094\n2          6.9  0.28       6.9   -3.2 -35095\n3          6.9  0.27       6.9   -3.1 -35094\n4          6.9  0.28       6.9   -3.2 -35094\n5          6.9  0.28       6.9   -3.2 -35093\n6          6.9  0.28       6.9   -3.2 -35093\n7          6.9  0.27       6.9   -3.1 -35095\n8          6.9  0.28       6.9   -3.2 -35094\n9          6.9  0.28       6.9   -3.2 -35094\n10         6.9  0.28       6.9   -3.2 -35096\n# ... with 3990 more draws\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n```\n\n\n:::\n:::\n\n\n## Plot the MCMC draws: $\\mu$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt_logn_draws |> \n  ggplot(aes(b_Intercept)) +\n  geom_density(alpha = 0.5, fill = \"darkgreen\") +\n  labs(x = expression(mu~\"of RTs (logged)\"))\n```\n\n::: {.cell-output-display}\n![Posterior probability distribution of $\\mu$ of RTs.](02-lognormal_files/figure-revealjs/fig-rt-logn-intercept-1.png){#fig-rt-logn-intercept width=960}\n:::\n:::\n\n\n## Plot the MCMC draws: $\\mu$ (ms)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt_logn_draws |> \n  ggplot(aes(exp(b_Intercept))) +\n  geom_density(alpha = 0.5, fill = \"darkgreen\") +\n  labs(x = expression(mu~\"of RTs (ms)\"))\n```\n\n::: {.cell-output-display}\n![Posterior probability distribution of $\\mu$ of RTs in ms.](02-lognormal_files/figure-revealjs/fig-rt-logn-intercept-ms-1.png){#fig-rt-logn-intercept-ms width=960}\n:::\n:::\n\n\n## Summarise the MCMC draws: $\\mu$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt_logn_draws |> \n  summarise(\n    mu_mean = round(mean(b_Intercept), 2),\n    mu_sd = round(sd(b_Intercept), 3)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  mu_mean mu_sd\n    <dbl> <dbl>\n1    6.88 0.004\n```\n\n\n:::\n:::\n\n\n## Summarise the MCMC draws: $\\mu$ (ms)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt_logn_draws |> \n  summarise(\n    mu_mean = round(mean(exp(b_Intercept))),\n    mu_sd = round(sd(exp(b_Intercept)))\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  mu_mean mu_sd\n    <dbl> <dbl>\n1     970     4\n```\n\n\n:::\n:::\n\n\n::: box-note\n-   The mean RTs is on average 970 ms (SD = 4).\n\n-   Always conditional on the model and the data.\n:::\n\n::: notes\n$$\n\\text{SD}_{\\text{natural}} = \\sqrt{ \\left( e^{\\sigma^2} - 1 \\right) \\cdot e^{2\\mu + \\sigma^2} }\n$$\n:::\n\n## Credible Intervals\n\n::: box-note\n-   Uncertainty can be quantified with Bayesian **Credible Intervals** (CrIs).\n\n-   A 95% CrI means that we can be 95% confident that the value is within the interval.\n:::\n\n. . .\n\n::: box-warning\nFrequentist Confidence Intervals are often wrongly interpreted as Bayesian Credible Intervals [@tan2010; @foster2014; @crooks2019; @gigerenzer2004; @cassidy2019].\n:::\n\n. . .\n\n::: box-note\n-   There is nothing special about 95%. You should obtain several.\n:::\n\n## Credible Intervals: higher level = larger width\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Several quantiles of a Gaussian distribution.](02-lognormal_files/figure-revealjs/fig-cris-1.png){#fig-cris width=960}\n:::\n:::\n\n\n## Calculate CrIs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(posterior)\n\nrt_logn_draws |> \n  mutate(\n    mu_ms = exp(b_Intercept)\n  ) |> \n  summarise(\n    hi_95 = quantile2(mu_ms, probs = 0.025),\n    lo_95 = quantile2(mu_ms, probs = 0.975),\n    hi_80 = quantile2(mu_ms, probs = 0.1),\n    lo_80 = quantile2(mu_ms, probs = 0.9),\n    hi_60 = quantile2(mu_ms, probs = 0.2),\n    lo_60 = quantile2(mu_ms, probs = 0.8),\n  ) |> \n  mutate(across(everything(), round))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  hi_95 lo_95 hi_80 lo_80 hi_60 lo_60\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1   962   977   965   975   967   973\n```\n\n\n:::\n:::\n\n\n## Report\n\n> We fitted a Bayesian log-normal model of reaction times (RTs) with brms [@burkner2017] in R [@rcoreteam2025].\n>\n> According to the model, the mean RT is between 962 and 977 ms, at 95% confidence (in logged ms, \\\\(\\\\beta\\\\) = 6.88, SD = 0.004). At 80% probability, the mean RT is 965-975 ms, while at 60% probability it is 967-973 ms.\n\n## Summary\n\n::: box-tip\n-   Gaussian variables are rare.\n\n-   Variables that are bounded to positive (real) numbers only usually follow a log-normal distribution. For example reaction times.\n\n-   Log-normal models estimate the mean and standard deviation of log-normal variables.\n\n$$\ny \\sim LogNormal(\\mu, \\sigma)\n$$\n:::\n\n## References\n",
    "supporting": [
      "02-lognormal_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}