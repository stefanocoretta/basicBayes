{
  "hash": "42e190e35cef31a0d0321ecfda8da24f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"01 - Introduction\"\nauthor: \"Stefano Coretta\"\n---\n\n## Disclaimer\n\n::: box-error\n- This workshop introduces you to the very basics of Bayesian regression modelling.\n\n- To be able to able to analyse real-world data you will need much more!\n:::\n\n## \n\n> The numbers have no way of speaking for themselves. We speak for them. We imbue them with meaning.\n\n— Nate Silver, *The Signal and the Noise*\n\n## Inference process\n\n![The inference process.](../img/inference.png)\n\n::: notes\nWe take a **sample** from the population.\n\nThis is our empirical data (the product of observation).\n\nHow do we go from data/observation to answering our question?\n\nWe can use **inference**.\n\n**Inference** is the process of understanding something about a population based on the sample (aka the data) taken from that population.\n:::\n\n## Fallibility\n\n::: box-note\n-   However, inference based on data does not guarantee infallible answers.\n\n-   In fact, any observation we make comes with a degree of **uncertainty and variability**.\n:::\n\n. . .\n\n::: box-tip\n-   Any one observation is uncertain.\n\n-   There aren't any two observations that are identical.\n:::\n\n## Uncertainty and variability\n\n![](../img/pliny.jpg)\n\n## \n\n![Guess what this is...](../img/uncertainty.png)\n\n## *Statistics* as a tool\n\n::: box-note\nStatistics helps us to **quantify uncertainty** and **account for variability**.\n:::\n\n## Statistics\n\n![](../img/data-quant.png)\n\n## Statistical model\n\n::: box-note\nA **statistical model** is a mathematical model that represents the relationship between variables in the data.\n:::\n\n. . .\n\n<br>\n\n> All models are wrong, but some are useful.\n\n—[George Box](https://en.wikipedia.org/wiki/All_models_are_wrong)\n\n## The path ahead\n\n##  {background-color=\"var(--inverse)\"}\n\n::: {style=\"font-size: 3em;\"}\nWhat is the mean reaction time in an auditory lexical decision task with?\n:::\n\n## The MALD data set\n\n::: box-note\n**Massive Auditory Lexical Decision** data set @tucker2019.\n\n-   MALD data set:\n\n    -   Lexical decision task in English.\n\n    -   Stimuli presented aurally.\n\n    -   521 subjects.\n\n-   Subset of MALD\n\n    -   30 subjects, 100 observations each.\n:::\n\n## MALD: the data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmald <- readRDS(\"data/tucker2019/mald_1_1.rds\")\n\nmald\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5,000 × 7\n   Subject Item        IsWord PhonLev    RT ACC     RT_log\n   <chr>   <chr>       <fct>    <dbl> <int> <fct>    <dbl>\n 1 15308   acreage     TRUE      6.01   617 correct   6.42\n 2 15308   maxraezaxr  FALSE     6.78  1198 correct   7.09\n 3 15308   prognosis   TRUE      8.14   954 correct   6.86\n 4 15308   giggles     TRUE      6.22   579 correct   6.36\n 5 15308   baazh       FALSE     6.13  1011 correct   6.92\n 6 15308   unflagging  TRUE      7.66  1402 correct   7.25\n 7 15308   ihnpaykaxrz FALSE     7.47  1059 correct   6.97\n 8 15308   hawk        TRUE      6.09   739 correct   6.61\n 9 15308   assessing   TRUE      6.37   789 correct   6.67\n10 15308   mehlaxl     FALSE     5.80   926 correct   6.83\n# ℹ 4,990 more rows\n```\n\n\n:::\n:::\n\n\n## Mean RT (with standard deviation)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmald |> \n  summarise(\n    mean_rt = round(mean(RT)),\n    sd_rt = round(sd(RT))\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  mean_rt sd_rt\n    <dbl> <dbl>\n1    1010   318\n```\n\n\n:::\n:::\n\n\n##  {background-color=\"var(--inverse)\"}\n\n::: {style=\"font-size: 3em;\"}\nAre you happy with the answer?\n:::\n\n## Problem: uncertainty and variability\n\n::: box-note\n-   These are the *sample* mean and standard deviation (SD).\n\n-   But what about the *population* mean and SD?\n:::\n\n. . .\n\n::: box-tip\n-   In the face of uncertainty and variability, we can use probability distributions.\n\n-   A family of probability distributions that can be identified (i.e. summarised) by a mean and SD is the **Gaussian** (aka \"normal\") **distribution** family (Gaussian distribution for short).\n:::\n\n## Gaussian distribution\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Gaussian distributions with varying mean and SD.](01-introduction_files/figure-revealjs/fig-gaussian-1.png){#fig-gaussian width=960}\n:::\n:::\n\n\n## Reaction Times\n\n$$\nRT \\sim Gaussian(\\mu, \\sigma)\n$$\n\n::: box-note\n-   Reaction Times ($RT$) are distributed according to ($\\sim$) a Gaussian distribution ($Gaussian()$) with mean $\\mu$ and SD $\\sigma$.\n:::\n\n. . .\n\n$$\n\\begin{align}\n\\mu & \\sim P(\\mu_1, \\sigma_1)\\\\\n\\sigma & \\sim P(\\mu_2, \\sigma_2)\n\\end{align}\n$$\n\n::: box-note\n-   The mean $\\mu$ comes from an undefined probability distribution $P()$ with its own mean $\\mu_1$ and SD $\\sigma_1$.\n\n-   The SD $\\sigma$ comes from an undefined probability distribution $P()$ with its own mean $\\mu_2$ and SD $\\sigma_2$.\n:::\n\n## A Gaussian model of RTs\n\n$$\n\\begin{align}\nRT & \\sim Gaussian(\\mu, \\sigma)\\\\\n\\mu & \\sim P(\\mu_1, \\sigma_1)\\\\\n\\sigma & \\sim P(\\mu_2, \\sigma_2)\n\\end{align}\n$$\n\n::: box-note\n-   We can fit a Bayesian Gaussian model of RT.\n\n-   The model estimates four parameters from the data: $\\mu_1, \\sigma_1, \\mu_2, \\sigma_2$.\n:::\n\n## Fitting Bayesian models in R\n\n::: box-tip\n-   Bayesian (regression) models can be fitted using the R [@rcoreteam2025] package **brms** @burkner2017.\n\n-   brms is an interface between R and **Stan** (a statistical software written in C++).\n:::\n\n. . .\n\n::: box-note\n-   brms (and Stan) use a special algorithm to estimate the model's parameters.\n\n-   Markov Chain Monte Carlo, or **MCMC** for short.\n:::\n\n## A Gaussian model of RTs: the code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\n\nrt_gauss <- brm(\n  RT ~ 1,\n  family = gaussian,\n  data = mald\n)\n```\n:::\n\n\n. . .\n\n```         \nCompiling Stan program...\nStart sampling\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000186 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.86 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.518 seconds (Warm-up)\nChain 1:                0.307 seconds (Sampling)\nChain 1:                0.825 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.71 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.667 seconds (Warm-up)\nChain 2:                0.264 seconds (Sampling)\nChain 2:                0.931 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6.7e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.67 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.574 seconds (Warm-up)\nChain 3:                0.333 seconds (Sampling)\nChain 3:                0.907 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6.7e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.67 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.458 seconds (Warm-up)\nChain 4:                0.334 seconds (Sampling)\nChain 4:                0.792 seconds (Total)\nChain 4: \n```\n\n\n::: {.cell}\n\n:::\n\n\n## A Gaussian model of RTs: plot $P()$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Posterior distributions of $\\mu$ and $\\sigma$ of RTs from the `mald` data.](01-introduction_files/figure-revealjs/fig-rt-gauss-mcmc-dens-1.png){#fig-rt-gauss-mcmc-dens width=960}\n:::\n:::\n\n\n## A Gaussian model of RTs: model summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(rt_gauss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ 1 \n   Data: mald (Number of observations: 5000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept  1010.39      4.43  1001.86  1018.84 1.00     3404     2751\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   317.83      3.14   311.83   324.28 1.00     3221     2530\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Is a Gaussian model a good choice?\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(rt_gauss, ndraws = 20)\n```\n\n::: {.cell-output-display}\n![Posterior predictive checks of `rt_gauss`.](01-introduction_files/figure-revealjs/fig-rt-gauss-pp-1.png){#fig-rt-gauss-pp width=960}\n:::\n:::\n\n\n## Summary\n\n::: box-tip\n- Inference allows us to learn something about a population from a sample of that population.\n\n- Statistics is a tool to quantify uncertainty and account for variability.\n\n- Gaussian models are a statistical tool that can estimate a mean and standard deviation from the data.\n\n$$\ny \\sim Gaussian(\\mu, \\sigma)\n$$\n:::\n\n## References\n",
    "supporting": [
      "01-introduction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}